# GDANN

--------------
Demo of GDANN (https://www.mdpi.com/1424-8220/21/7/2369/htm#)



## Getting Started

---------------

1. Go to **prepocess** folder to see how to do the preprocess work using **PSD**.
2. Go to **classification** folder to see how to run the classifiction work using **GDANN**.



## Citation

-------

If you use this code for your research, please cite our papers.

>@Article{s21072369,\
AUTHOR = {Zeng, Hong and Li, Xiufeng and Borghini, Gianluca and Zhao, Yue and Aricò, Pietro and Di Flumeri, Gianluca and Sciaraffa, Nicolina and Zakaria, Wael and Kong, Wanzeng and Babiloni, Fabio},\
TITLE = {An EEG-Based Transfer Learning Method for Cross-Subject Fatigue Mental State Prediction},\
JOURNAL = {Sensors},\
VOLUME = {21},\
YEAR = {2021},\
NUMBER = {7},\
ARTICLE-NUMBER = {2369},\
URL = {https://www.mdpi.com/1424-8220/21/7/2369}, \
ISSN = {1424-8220},\
ABSTRACT = {Fatigued driving is one of the main causes of traffic accidents. The electroencephalogram (EEG)-based mental state analysis method is an effective and objective way of detecting fatigue. However, as EEG shows significant differences across subjects, effectively “transfering” the EEG analysis model of the existing subjects to the EEG signals of other subjects is still a challenge. Domain-Adversarial Neural Network (DANN) has excellent performance in transfer learning, especially in the fields of document analysis and image recognition, but has not been applied directly in EEG-based cross-subject fatigue detection. In this paper, we present a DANN-based model, Generative-DANN (GDANN), which combines Generative Adversarial Networks (GAN) to enhance the ability by addressing the issue of different distribution of EEG across subjects. The comparative results show that in the analysis of cross-subject tasks, GDANN has a higher average accuracy of 91.63% in fatigue detection across subjects than those of traditional classification models, which is expected to have much broader application prospects in practical brain–computer interaction (BCI).},\
DOI = {10.3390/s21072369}\
}



